# -*- coding: utf-8 -*-
"""Untitled1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1LpLGRiUc7J7hSQMYAEsjrCrWa06hXnPD
"""

#experiment 1
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score

X = np.array([1, 2, 3, 4, 5, 6]).reshape(-1, 1)
Y = np.array([2, 4, 5, 4, 5, 7])

X_train, X_test, y_train, y_test = train_test_split(X, Y,
test_size=0.2, random_state=42)

model = LinearRegression()
model.fit(X_train, y_train)

y_pred = model.predict(X_test)

mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)
print("Mean Squared Error:", mse)
print("R2 Score:", r2)
print("Slope (β1):", model.coef_[0])
print("Intercept (β0):", model.intercept_)

plt.scatter(X, Y, color='blue', label='Actual Data')
plt.plot(X, model.predict(X), color='red', label='Regression Line')
plt.xlabel("X")
plt.ylabel("Y")
plt.title("Linear Regression Example")
plt.legend()
plt.show()

# Experiment 2: Decision Tree Classifier and Visualization

from sklearn.datasets import load_iris
from sklearn.tree import DecisionTreeClassifier, plot_tree
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
import matplotlib.pyplot as plt


iris = load_iris()
X, y = iris.data, iris.target

X_train, X_test, y_train, y_test = train_test_split(X, y,
test_size=0.3, random_state=42)


model = DecisionTreeClassifier(criterion='entropy', max_depth=3,
random_state=0)
model.fit(X_train, y_train)


y_pred = model.predict(X_test)


acc = accuracy_score(y_test, y_pred)
print("Decision Tree Accuracy:", acc)


plt.figure(figsize=(12, 8))
plot_tree(model,
          feature_names=iris.feature_names,
          class_names=iris.target_names,
          filled=True,
          rounded=True)
plt.title("Decision Tree Visualization (Iris Dataset)")
plt.show()

# Experiment 3: Demonstration of Overfitting and Regularization (L1 & L2)
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression, Ridge, Lasso
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score


np.random.seed(42)
X = np.linspace(0, 10, 100).reshape(-1, 1)
y = 3 * X.squeeze() + np.random.randn(100) * 3

X_train, X_test, y_train, y_test = train_test_split(X, y,
test_size=0.2, random_state=42)

lr = LinearRegression().fit(X_train, y_train)
ridge = Ridge(alpha=1.0).fit(X_train, y_train)
lasso = Lasso(alpha=0.1).fit(X_train, y_train)

y_pred_lr = lr.predict(X_test)
y_pred_ridge = ridge.predict(X_test)
y_pred_lasso = lasso.predict(X_test)

print("Linear Regression R²:", r2_score(y_test, y_pred_lr))
print("Ridge Regression R²:", r2_score(y_test, y_pred_ridge))
print("Lasso Regression R²:", r2_score(y_test, y_pred_lasso))

plt.scatter(X, y, color='gray', label='Data')
plt.plot(X_test, y_pred_lr, color='blue', label='Linear Regression')
plt.plot(X_test, y_pred_ridge, color='green', label='Ridge (L2)')
plt.plot(X_test, y_pred_lasso, color='red', label='Lasso (L1)')
plt.xlabel("X")
plt.ylabel("Y")
plt.legend()
plt.title("Effect of L1 and L2 Regularization")
plt.show()

# Experiment 4: k-Nearest Neighbors (k-NN)

from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score
import matplotlib.pyplot as plt
import numpy as np

iris = load_iris()
X, y = iris.data[:, :2], iris.target


scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

X_train, X_test, y_train, y_test = train_test_split(X_scaled, y,
test_size=0.3, random_state=42)

k = 3
model = KNeighborsClassifier(n_neighbors=k)
model.fit(X_train, y_train)

y_pred = model.predict(X_test)

accuracy = accuracy_score(y_test, y_pred)
print("k-NN Classifier Accuracy with k =", k, ":", accuracy)

h = 0.02
x_min, x_max = X_scaled[:, 0].min() - 1, X_scaled[:, 0].max() + 1
y_min, y_max = X_scaled[:, 1].min() - 1, X_scaled[:, 1].max() + 1
xx, yy = np.meshgrid(np.arange(x_min, x_max, h),
np.arange(y_min, y_max, h))
Z = model.predict(np.c_[xx.ravel(), yy.ravel()])
Z = Z.reshape(xx.shape)
plt.contourf(xx, yy, Z, alpha=0.8)
plt.scatter(X_scaled[:, 0], X_scaled[:, 1], c=y, edgecolors='k',
marker='o', s=50, cmap=plt.cm.Paired)
plt.title(f'k-NN Decision Boundary (k={k})')
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
plt.show()

#Experiment 5: Naive Bayes Classifier for Text Classification
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import accuracy_score
X_train = ["Win money now", "Hello friend"]
y_train = ["spam", "ham"]
X_test = ["Limited offer", "Let's meet tomorrow"]
y_test = ["spam", "ham"]
vectorizer = CountVectorizer()
X_train_vec = vectorizer.fit_transform(X_train)
X_test_vec = vectorizer.transform(X_test)
model = MultinomialNB()
model.fit(X_train_vec, y_train)
y_pred = model.predict(X_test_vec)
accuracy = accuracy_score(y_test, y_pred)
print("Predictions:", y_pred)
print("Accuracy:", accuracy)



# Experiment 6: Logistic Regression for Binary Classification

import numpy as np
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, confusion_matrix


X_train = np.array([[1], [2], [3], [4]])
y_train = np.array([0, 0, 0, 1])


X_test = np.array([[2], [5]])
y_test = np.array([0, 1])

model = LogisticRegression(solver='liblinear', C=1000)

model.fit(X_train, y_train)

y_pred = model.predict(X_test)

acc = accuracy_score(y_test, y_pred)
cm = confusion_matrix(y_test, y_pred)

print("Predicted Labels:", y_pred)
print("Accuracy:", acc)
print("Confusion Matrix:")
print(cm)

# Experiment 7: Logistic Regression for Binary Classification

import numpy as np
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, confusion_matrix

X = np.array([[1], [2], [3], [4], [5], [6]])
y = np.array([0, 0, 0, 1, 1, 1])

X_train = np.array([[1], [2], [5], [6]])
y_train = np.array([0, 0, 1, 1])

X_test = np.array([[3], [4]])
y_test = np.array([0, 1])

model = LogisticRegression(solver='liblinear')

model.fit(X_train, y_train)


y_pred = model.predict(X_test)

acc = accuracy_score(y_test, y_pred)
cm = confusion_matrix(y_test, y_pred)

print("Predicted Labels:", y_pred)
print("Accuracy:", acc)
print("Confusion Matrix:")
print(cm)

#Experiment 8: SVM with Linear and RBF Kernels
import numpy as np
from sklearn import dataset
from sklearn.preprocessing import StandardScaler
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, confusion_matrix

iris = datasets.load_iris()
X = iris.data[:100, 2:3]
y = iris.target[:100]

X_train = np.vstack((X[:35], X[50:85]))
y_train = np.hstack((y[:35], y[50:85]))

X_test = np.vstack((X[35:50], X[85:100]))
y_test = np.hstack((y[35:50], y[85:100]))

scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

svm_linear = SVC(kernel='linear', C=1.0)
svm_linear.fit(X_train, y_train)
y_pred_linear = svm_linear.predict(X_test)

svm_rbf = SVC(kernel='rbf', gamma=0.5, C=1.0)
svm_rbf.fit(X_train, y_train)
y_pred_rbf = svm_rbf.predict(X_test)

print("Linear Kernel Accuracy:", accuracy_score(y_test, y_pred_linear))
print("RBF Kernel Accuracy:", accuracy_score(y_test, y_pred_rbf))

print("Confusion Matrix (Linear):")
print(confusion_matrix(y_test, y_pred_linear))

print("Confusion Matrix (RBF):")
print(confusion_matrix(y_test, y_pred_rbf))

#Experiment 9: Clustering using k-Means and Gaussian Mixture Model

import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import make_blobs
from sklearn.cluster import KMeans
from sklearn.mixture import GaussianMixture

X, _ = make_blobs(n_samples=300, centers=3, cluster_std=0.6,
random_state=0)

kmeans = KMeans(n_clusters=3, random_state=0)
y_kmeans = kmeans.fit_predict(X)

gmm = GaussianMixture(n_components=3, random_state=0)
y_gmm = gmm.fit_predict(X)

plt.figure(figsize=(10,4))

plt.subplot(1,2,1)
plt.scatter(X[:, 0], X[:, 1], c=y_kmeans, s=40, cmap='viridis')
plt.title("k-Means Clustering")
plt.xlabel("Feature 1")
plt.ylabel("Feature 2")

plt.subplot(1,2,2)
plt.scatter(X[:, 0], X[:, 1], c=y_gmm, s=40, cmap='viridis')
plt.title("Gaussian Mixture Model Clustering")
plt.xlabel("Feature 1")
plt.ylabel("Feature 2")
plt.tight_layout()
plt.show()

#exp 10 random forest
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier
from sklearn.metrics import accuracy_score, confusion_matrix

iris = load_iris()
X = iris.data
y = (iris.target != 0) * 1

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=42
)

rf = RandomForestClassifier(n_estimators=100, random_state=42)
rf.fit(X_train, y_train)
y_pred_rf = rf.predict(X_test)

ada = AdaBoostClassifier(n_estimators=50, random_state=42)
ada.fit(X_train, y_train)
y_pred_ada = ada.predict(X_test)

acc_rf = accuracy_score(y_test, y_pred_rf)
acc_ada = accuracy_score(y_test, y_pred_ada)

print("Random Forest Accuracy:", acc_rf)
print("AdaBoost Accuracy:", acc_ada)

print("\nConfusion Matrix (Random Forest):")
print(confusion_matrix(y_test, y_pred_rf))

print("\nConfusion Matrix (AdaBoost):")
print(confusion_matrix(y_test, y_pred_ada))